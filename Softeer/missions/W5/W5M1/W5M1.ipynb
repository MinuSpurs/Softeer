{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4dac001-fbb1-473f-b329-1b3c21ff5e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark 세션 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC TLC Trip Analysis - RDD\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66bdd364-b02d-4541-8537-f4623df6b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/data/yellow_tripdata_2025-06.parquet\"\n",
    "\n",
    "# Parquet 파일을 DataFrame으로 읽기\n",
    "df = spark.read.parquet(file_path)\n",
    "\n",
    "# RDD로 변환\n",
    "rdd = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca78814c-6225-4f19-b04e-fc98314445f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 컬럼 추출: (pickup_date, fare_amount, trip_distance)\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df = df.withColumn(\"pickup_date\", to_date(df.tpep_pickup_datetime))  # 날짜만 추출\n",
    "\n",
    "rdd = df.select(\"pickup_date\", \"fare_amount\", \"trip_distance\") \\\n",
    "        .rdd \\\n",
    "        .filter(lambda row: row[\"fare_amount\"] > 0 and row[\"trip_distance\"] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a220039d-59fe-45ce-b513-f8b6c2c62de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 총 운행 수\n",
    "total_trips = rdd.count()\n",
    "\n",
    "# 총 수익\n",
    "total_revenue = rdd.map(lambda row: row[\"fare_amount\"]).sum()\n",
    "\n",
    "# 평균 거리\n",
    "avg_distance = rdd.map(lambda row: row[\"trip_distance\"]).mean()\n",
    "\n",
    "# 일별 운행 수\n",
    "trips_per_day = rdd.map(lambda row: (row[\"pickup_date\"], 1)).reduceByKey(lambda a, b: a + b).collect()\n",
    "\n",
    "# 일별 수익\n",
    "revenue_per_day = rdd.map(lambda row: (row[\"pickup_date\"], row[\"fare_amount\"])).reduceByKey(lambda a, b: a + b).collect()\n",
    "\n",
    "# 실제 운행 날 수\n",
    "distinct_dates = rdd.map(lambda row: row[\"pickup_date\"]).distinct()\n",
    "\n",
    "# 전체 RDD 에서 10%만 샘플링\n",
    "sampled_rdd = rdd.sample(withReplacement=False, fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "803d19cf-9409-4d1e-aae6-3131f50c7711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "summary = spark.createDataFrame(\n",
    "    [Row(metric=\"total_trips\", value=float(total_trips))] +\n",
    "    [Row(metric=\"total_revenue\", value=float(total_revenue))] +\n",
    "    [Row(metric=\"avg_distance\", value=float(avg_distance))] +\n",
    "    [Row(metric=f\"daily_trips_{str(k)}\", value=float(v)) for k, v in trips_per_day] +\n",
    "    [Row(metric=f\"daily_revenue_{str(k)}\", value=float(v)) for k, v in revenue_per_day] +\n",
    "    [Row(metric=\"distinct_dates\", value=str(distinct_dates))] +\n",
    "    [Row(metric=\"sampled_rdd\", value=\"RDD object (not serializable)\")]\n",
    ")\n",
    "\n",
    "summary.write.csv(\"/output/summary\", header=True, mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
